{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training Transformer\n\n### TA's Slide\n[Slide](https://docs.google.com/presentation/d/1ga0d43mWyrfHjdkp7FG3iWcEKTGr8CdkifYhhi4LBY8/edit?usp=sharing)\n\n### Description\nIn this assignment, we are tasked with utilizing a transformer decoder-only architecture for pretraining, with a focus on next-token prediction, applied to Pokémon images.\n\nPlease feel free to mail us if you have any questions.\n\nntu-ml-2025-spring-ta@googlegroups.com","metadata":{"id":"QUwfibz3pw_R"}},{"cell_type":"markdown","source":"# Utilities","metadata":{"id":"8TSNIY5SAZUR"}},{"cell_type":"markdown","source":"### Download packages","metadata":{"id":"LlFjd2Jya7Me"}},{"cell_type":"code","source":"!pip install datasets==3.3.2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fLdY0QcGsfii","outputId":"95264a30-2cdf-420a-ec75-a4d175cba14c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets==3.3.2\n","  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==3.3.2) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.3.2) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.3.2) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets==3.3.2)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==3.3.2) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==3.3.2) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets==3.3.2) (4.67.1)\n","Collecting xxhash (from datasets==3.3.2)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets==3.3.2)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets==3.3.2)\n","  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==3.3.2) (3.11.15)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.3.2) (0.30.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==3.3.2) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==3.3.2) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.3.2) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.3.2) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.3.2) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.3.2) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.3.2) (6.3.1)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.3.2) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.3.2) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets==3.3.2) (4.13.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.3.2) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.3.2) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.3.2) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.3.2) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.3.2) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.3.2) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.3.2) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.3.2) (1.17.0)\n","Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.2\n","    Uninstalling fsspec-2025.3.2:\n","      Successfully uninstalled fsspec-2025.3.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.3.2 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"]}],"execution_count":1},{"cell_type":"markdown","source":"### Import Packages","metadata":{"id":"kY7XIt-Msf5K"}},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom PIL import Image\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nfrom transformers import AutoModelForCausalLM, GPT2Config, set_seed\nfrom datasets import load_dataset\nfrom typing import Dict, Any, Optional","metadata":{"id":"OJ_4mAauKFu1"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import LlamaConfig, LlamaForCausalLM","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Check Devices","metadata":{"id":"WixuPQuGbG_z"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"9COb5XCNbJA_"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Set Random Seed","metadata":{"id":"plHUxP_oeh2U"}},{"cell_type":"code","source":"set_seed(0)","metadata":{"id":"Z3YIraV3ehgv"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare Data","metadata":{"id":"w4p3ErI1AaYq"}},{"cell_type":"markdown","source":"### Define Dataset","metadata":{"id":"1J0xr161bREA"}},{"cell_type":"code","source":"from typing import List, Tuple, Union\nimport torch\nfrom torch.utils.data import Dataset\n\nclass PixelSequenceDataset(Dataset):\n    def __init__(self, data: List[List[int]], mode: str = \"train\"):\n        \"\"\"\n        A dataset class for handling pixel sequences.\n\n        Args:\n            data (List[List[int]]): A list of sequences, where each sequence is a list of integers.\n            mode (str): The mode of operation, either \"train\", \"dev\", or \"test\".\n                - \"train\": Returns (input_ids, labels) where input_ids are sequence[:-1] and labels are sequence[1:].\n                - \"dev\": Returns (input_ids, labels) where input_ids are sequence[:-160] and labels are sequence[-160:].\n                - \"test\": Returns only input_ids, as labels are not available.\n        \"\"\"\n        self.data = data\n        self.mode = mode\n\n    def __len__(self) -> int:\n        \"\"\"Returns the total number of sequences in the dataset.\"\"\"\n        return len(self.data)\n\n    def __getitem__(self, idx: int) -> Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n        \"\"\"\n        Fetches a sequence from the dataset and processes it based on the mode.\n\n        Args:\n            idx (int): The index of the sequence.\n\n        Returns:\n            - If mode == \"train\": Tuple[torch.Tensor, torch.Tensor] -> (input_ids, labels)\n            - If mode == \"dev\": Tuple[torch.Tensor, torch.Tensor] -> (input_ids, labels)\n            - If mode == \"test\": torch.Tensor -> input_ids\n        \"\"\"\n        sequence = self.data[idx]\n\n        if self.mode == \"train\":\n            input_ids = torch.tensor(sequence[:-1], dtype=torch.long)\n            labels = torch.tensor(sequence[1:], dtype=torch.long)\n            return input_ids, labels\n\n        elif self.mode == \"dev\":\n            input_ids = torch.tensor(sequence[:-160], dtype=torch.long)\n            labels = torch.tensor(sequence[-160:], dtype=torch.long)\n            return input_ids, labels\n\n        elif self.mode == \"test\":\n            input_ids = torch.tensor(sequence, dtype=torch.long)\n            return input_ids\n\n        raise ValueError(f\"Invalid mode: {self.mode}. Choose from 'train', 'dev', or 'test'.\")","metadata":{"id":"Bi9YvYn3uD32"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Download Dataset & Prepare Dataloader","metadata":{"id":"zY6TJDuNbdTr"}},{"cell_type":"code","source":"# Load the pokemon dataset from Hugging Face Hub\npokemon_dataset = load_dataset(\"lca0503/ml2025-hw4-pokemon\")\n\n# Load the colormap from Hugging Face Hub\ncolormap = list(load_dataset(\"lca0503/ml2025-hw4-colormap\")[\"train\"][\"color\"])\n\n# Define number of classes\nnum_classes = len(colormap)\n\n# Define batch size\nbatch_size = 16\n\n# === Prepare Dataset and DataLoader for Training ===\ntrain_dataset: PixelSequenceDataset = PixelSequenceDataset(\n    pokemon_dataset[\"train\"][\"pixel_color\"], mode=\"train\"\n)\ntrain_dataloader: DataLoader = DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True\n)\n\n# === Prepare Dataset and DataLoader for Validation ===\ndev_dataset: PixelSequenceDataset = PixelSequenceDataset(\n    pokemon_dataset[\"dev\"][\"pixel_color\"], mode=\"dev\"\n)\ndev_dataloader: DataLoader = DataLoader(\n    dev_dataset, batch_size=batch_size, shuffle=False\n)\n\n# === Prepare Dataset and DataLoader for Testing ===\ntest_dataset: PixelSequenceDataset = PixelSequenceDataset(\n    pokemon_dataset[\"test\"][\"pixel_color\"], mode=\"test\"\n)\ntest_dataloader: DataLoader = DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=False\n)","metadata":{"id":"lHXfCPXoBEUD"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualization","metadata":{"id":"Ink91WB5BGxv"}},{"cell_type":"code","source":"def pixel_to_image(pixel_color: List[int], colormap: List[List[int]]) -> Image.Image:\n    \"\"\"\n    Converts a list of pixel indices into a 20x20 RGB image using a colormap.\n\n    Args:\n        pixel_color (List[int]): A list of pixel indices representing colors.\n        colormap (List[List[int]]): A list where each index maps to an RGB color [R, G, B].\n\n    Returns:\n        Image.Image: A PIL Image object representing the reconstructed image.\n    \"\"\"\n    # Ensure the pixel_color list has at least 400 elements (pad with 0s if needed)\n    while len(pixel_color) < 400:\n        pixel_color.append(0)\n\n    # Map pixel indices to actual RGB colors using the colormap\n    pixel_data = [colormap[pixel] for pixel in pixel_color]\n\n    # Convert to numpy array and reshape to 20x20x3 (RGB image)\n    image_array = np.array(pixel_data, dtype=np.uint8).reshape(20, 20, 3)\n\n    # Create a PIL Image from the array\n    image = Image.fromarray(image_array)\n\n    return image\n\ndef show_images(images: List[Image.Image]) -> None:\n    \"\"\"\n    Displays a grid of up to 96 images using Matplotlib.\n\n    Args:\n        images (List[Image.Image]): A list of PIL Image objects to display.\n\n    Returns:\n        None\n    \"\"\"\n    num_images = min(96, len(images))  # Limit to 96 images\n\n    # Set up the figure size and grid layout (6 rows, 16 columns)\n    fig, axes = plt.subplots(6, 16, figsize=(16, 6))\n    axes = axes.flatten()  # Flatten to make iteration easier\n\n    # Loop through images and display each one in the grid\n    for i, ax in enumerate(axes):\n        if i < num_images:\n            ax.imshow(images[i])\n            ax.axis('off')  # Hide axis\n        else:\n            ax.axis('off')  # Hide unused subplots\n\n    plt.tight_layout()  # Adjust layout to prevent overlap\n    plt.show()","metadata":{"id":"ARCMjCUfBGV9"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize train images\ntrain_images = [pixel_to_image(data[\"pixel_color\"], colormap) for data in pokemon_dataset[\"train\"]]\nshow_images(train_images)","metadata":{"id":"706fqI8NBJGm"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize test images\ntest_images = [pixel_to_image(data[\"pixel_color\"], colormap) for data in pokemon_dataset[\"test\"]]\nshow_images(test_images)","metadata":{"id":"QtllXoTLBKXk"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare Model","metadata":{"id":"wjbJUIxzBNEH"}},{"cell_type":"markdown","source":"### Model Configuration\nHere, we define the model configuration, including the architecture and key hyperparameters such as the number of attention heads, layers, embedding size, and more.\n*   Hint 1: Adjust hyperparameters here for improved performance.\n*   Hint 2: Experiment with different model architectures, such as Llama, Mistral, or Qwen, to enhance performance.\n  * [LlamaConfig](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaConfig)\n  * [MistralConfig](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralConfig)\n  * [Qwen2Config](https://huggingface.co/docs/transformers/model_doc/qwen2#transformers.Qwen2Config)\n","metadata":{"id":"o541odctzSc9"}},{"cell_type":"code","source":"# # Define GPT-2 model configuration as a dictionary\n# gpt2_config = {\n#     \"activation_function\": \"gelu_new\",    # Activation function used in the model\n#     \"architectures\": [\"GPT2LMHeadModel\"],  # Specifies the model type\n#     \"attn_pdrop\": 0.1,            # Dropout rate for attention layers\n#     \"embd_pdrop\": 0.1,            # Dropout rate for embeddings\n#     \"initializer_range\": 0.02,        # Standard deviation for weight initialization\n#     \"layer_norm_epsilon\": 1e-05,       # Small constant to improve numerical stability in layer norm\n#     \"model_type\": \"gpt2\",           # Type of model\n#     \"n_ctx\": 128,               # Context size (maximum sequence length)\n#     \"n_embd\": 64,              # Embedding size\n#     \"n_head\": 2,               # Number of attention heads\n#     \"n_layer\": 2,              # Number of transformer layers\n#     \"n_positions\": 400,           # Maximum number of token positions\n#     \"resid_pdrop\": 0.1,           # Dropout rate for residual connections\n#     \"vocab_size\": num_classes,       # Number of unique tokens in vocabulary\n#     \"pad_token_id\": None,          # Padding token ID (None means no padding token)\n#     \"eos_token_id\": None,          # End-of-sequence token ID (None means not explicitly defined)\n# }\n\n# # Load GPT-2 model configuration from dictionary\n# config = GPT2Config.from_dict(gpt2_config)","metadata":{"id":"pDpLtOZBzU3o"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config = LlamaConfig(\n    vocab_size=167,\n    hidden_size=512,\n    intermediate_size=2048,\n    num_hidden_layers=8,\n    num_attention_heads=8,\n    max_position_embeddings=400,\n    pad_token_id = None,\n    bos_token_id = None,\n    eos_token_id = None,\n)\nmodel = LlamaForCausalLM(config)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load Model","metadata":{"id":"JCOELUm4ujej"}},{"cell_type":"code","source":"# Load the model using the configuration defined above\n# model = AutoModelForCausalLM.from_config(config)\n\nprint(model)\n\n# Count trainable parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"Trainable Parameters: {trainable_params:,}\")","metadata":{"id":"4U4JxNF5CFG6"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train and Inference","metadata":{"id":"flJizztpCT01"}},{"cell_type":"markdown","source":"### Training Arguments\nHere, we define the number of epochs for training, the learning rate, the optimizer, and the loss function.\n*   Hint 3: Adjust the number of epochs and learning rate here to improve performance.","metadata":{"id":"d63rxnsCeHa9"}},{"cell_type":"code","source":"# Training Parameters\nepochs = 50                                      # Number of training epochs\nlearning_rate = 1e-3                                 # Learning rate for optimizer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")     # Check if CUDA is available for GPU\nsave_dir = \"checkpoints\"                               # Directory to save model checkpoints\n\n# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()                          # Loss function for classification tasks\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.1) # AdamW optimizer with weight decay","metadata":{"id":"oJu31QhHeKBj"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Save Model Function","metadata":{"id":"Xs2kjoD4d_bJ"}},{"cell_type":"code","source":"def save_model(model: torch.nn.Module, optimizer: torch.optim.Optimizer, epoch: int, loss: float, save_dir: str, filename: str = \"best_model.pth\") -> None:\n    \"\"\"\n    Saves the model state, optimizer state, current epoch, and loss to a specified directory.\n\n    Args:\n        model (torch.nn.Module): The PyTorch model to be saved.\n        optimizer (torch.optim.Optimizer): The optimizer whose state will be saved.\n        epoch (int): The current epoch number (used for checkpointing).\n        loss (float): The current loss value to track model performance.\n        save_dir (str): The directory where the model checkpoint will be saved.\n        filename (str, optional): The name of the file to save the model. Defaults to \"best_model.pth\".\n\n    Returns:\n        None\n    \"\"\"\n    # Construct the full path for saving the model checkpoint\n    save_path = os.path.join(save_dir, filename)\n\n    # Save the model, optimizer state, and additional metadata (epoch and loss)\n    torch.save({\n        'epoch': epoch + 1,                # Save epoch + 1 for easier tracking\n        'model_state_dict': model.state_dict(),       # Save model weights\n        'optimizer_state_dict': optimizer.state_dict(),  # Save optimizer state (important for resuming training)\n        'loss': loss                     # Save the current loss value\n    }, save_path)\n\n    # Print a confirmation message indicating the model has been saved\n    print(f\"Model saved at {save_path} (Loss: {loss:.4f}, Epoch: {epoch + 1})\")","metadata":{"id":"ZakIXlw_NtDw"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train\n\nWe save the checkpoint with the lowest training loss since validation set reconstruction accuracy doesn't directly reflect the model's image generation quality.\n*   Hint 4: Train a classifier to check if an image looks like a Pokémon or not. (Optional)","metadata":{"id":"I0OqYrJxoxx9"}},{"cell_type":"code","source":"# Create save directory if it doesn't exist\nos.makedirs(save_dir, exist_ok=True)\n# Initialize best loss as positive infinity for comparison during model checkpointing\nbest_loss: float = float('inf')\n# Move model to the appropriate device (GPU or CPU)\nmodel.to(device)\n\n# Training Loop\nfor epoch in range(epochs):\n    model.train()  # Set the model to training mode\n    epoch_loss = 0  # Initialize the epoch loss\n\n    # Iterate over training data batches\n    for input_ids, labels in tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n        input_ids, labels = input_ids.to(device), labels.to(device)  # Move data to the same device as the model\n\n        # Forward pass through the model to get logits (output probabilities)\n        outputs = model(input_ids=input_ids).logits.view(-1, config.vocab_size)\n        labels = labels.view(-1)  # Flatten labels to match logits shape\n\n        # Calculate loss using CrossEntropyLoss\n        loss = criterion(outputs, labels)\n\n        # Backpropagation and optimizer step\n        optimizer.zero_grad()  # Reset gradients to zero\n        loss.backward()     # Compute gradients\n        optimizer.step()     # Update model weights\n\n        # Accumulate the loss for the epoch\n        epoch_loss += loss.item()\n\n    # Compute average epoch loss\n    avg_epoch_loss = epoch_loss / len(train_dataloader)\n    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_epoch_loss:.4f}\")\n\n    # Evaluation Loop (Validation)\n    model.eval()      # Set the model to evaluation mode (disables dropout, etc.)\n    total_accuracy = 0  # Initialize total accuracy\n    num_batches = 0   # Initialize batch counter\n\n    with torch.no_grad():  # Disable gradient calculation for validation\n        # Iterate over validation data batches\n        for inputs, labels in tqdm(dev_dataloader, desc=\"Evaluating\"):\n            inputs, labels = inputs.to(device), labels.to(device)  # Move validation data to device\n            attention_mask = torch.ones_like(inputs)          # Attention mask to ensure valid token positions\n\n            # Perform batch inference using the model\n            generated_outputs = model.generate(inputs, attention_mask=attention_mask, max_length=400)\n\n            # Extract the last 160 tokens from generated outputs and labels\n            generated_outputs = generated_outputs[:, -160:]\n\n            # Calculate accuracy for the batch\n            accuracy = (generated_outputs == labels).float().mean().item()\n            total_accuracy += accuracy\n            num_batches += 1\n\n    # Compute average reconstruction accuracy for the epoch\n    avg_accuracy = total_accuracy / num_batches\n    print(f\"Epoch {epoch + 1}/{epochs}, Reconstruction Accuracy: {avg_accuracy:.4f}\")\n\n    # If the current epoch loss is better (lower) than the best loss, save the model\n    if avg_epoch_loss < best_loss:\n        best_loss = avg_epoch_loss                   # Update best loss\n        save_model(model, optimizer, epoch, best_loss, save_dir)  # Save the model with the best loss","metadata":{"id":"e1KHQ2UyCNh7"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Inference","metadata":{"id":"mNEXUdGBo5eK"}},{"cell_type":"code","source":"# Load the best model from the saved checkpoint\nbest_model_path = os.path.join(save_dir, \"best_model.pth\")              # Path to the best model checkpoint\ncheckpoint = torch.load(best_model_path, weights_only=True, map_location=device)  # Load checkpoint from the file\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])                  # Load the model weights from checkpoint\nmodel.eval()                                        # Set the model to evaluation mode (disables dropout, etc.)\n\n# Testing Loop with Batch Inference\nresults: list = []  # List to store the generated sequences from the model\n\nwith torch.no_grad():  # Disable gradient calculations for inference\n    # Iterate over test data in batches\n    for inputs in tqdm(test_dataloader, desc=\"Generating Outputs\"):\n        inputs = inputs.to(device)         # Move model to the appropriate device (GPU or CPU)\n        attention_mask = torch.ones_like(inputs)  # Attention mask (ensure valid token positions)\n\n        # Generate predictions for the entire batch\n        generated_outputs = model.generate(inputs, attention_mask=attention_mask, max_length=400)\n\n        # Convert batch outputs to a list and append to results\n        batch_results = generated_outputs.cpu().numpy().tolist()\n        results.extend(batch_results)  # Extend the results list with batch results\n\n# Save the results to a file\noutput_file: str = \"reconstructed_results.txt\"  # File to save the output sequences\nwith open(output_file, \"w\") as f:\n    # Write each sequence to the file\n    for seq in results:\n        f.write(\" \".join(map(str, seq)) + \"\\n\")\n\nprint(f\"Reconstructed results saved to {output_file}\")  # Confirmation message","metadata":{"id":"D-A5aGBRGR4q"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize generated test images\npredicted_images = [pixel_to_image(sequence, colormap) for sequence in results]\nshow_images(predicted_images)","metadata":{"id":"xPxOAnFHIxfb"},"outputs":[],"execution_count":null}]}