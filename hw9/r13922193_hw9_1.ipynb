{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKqn581krkSS"
   },
   "source": [
    "# **HW9 Model Merging**\n",
    "This is the Notebook of ML2025 HW9.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UO-FOq39HdZT"
   },
   "source": [
    "# Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T01:48:29.540989Z",
     "iopub.status.busy": "2025-06-06T01:48:29.540711Z",
     "iopub.status.idle": "2025-06-06T01:48:29.777342Z",
     "shell.execute_reply": "2025-06-06T01:48:29.776552Z",
     "shell.execute_reply.started": "2025-06-06T01:48:29.540965Z"
    },
    "id": "q4ol4QRnmw-O",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun  6 01:48:29 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   39C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   40C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check GPU Availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9x8nKdPZHyyX"
   },
   "source": [
    "# Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T01:48:29.779119Z",
     "iopub.status.busy": "2025-06-06T01:48:29.778871Z",
     "iopub.status.idle": "2025-06-06T01:50:11.051241Z",
     "shell.execute_reply": "2025-06-06T01:50:11.050224Z",
     "shell.execute_reply.started": "2025-06-06T01:48:29.779098Z"
    },
    "id": "4UT0EKepdxLI",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: torchvision==0.20.1 in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
      "Requirement already satisfied: torchaudio==2.5.1 in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (1.13.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.20.1) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.20.1) (11.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.20.1) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.20.1) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.20.1) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.20.1) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.20.1) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.20.1) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.20.1) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.20.1) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.20.1) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision==0.20.1) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision==0.20.1) (2024.2.0)\n",
      "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "# Install torch\n",
    "!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T01:50:11.052582Z",
     "iopub.status.busy": "2025-06-06T01:50:11.052301Z",
     "iopub.status.idle": "2025-06-06T01:50:15.877015Z",
     "shell.execute_reply": "2025-06-06T01:50:15.876232Z",
     "shell.execute_reply.started": "2025-06-06T01:50:11.052558Z"
    },
    "id": "ZDK2EXt3BEHK",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets==3.5.1\n",
      "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.1) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.1) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.1) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.1) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.1) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.1) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.1) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.1) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.1) (0.70.16)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.5.1)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.1) (3.11.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.1) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.1) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.1) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.1) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.1) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.1) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.1) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.1) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.1) (1.19.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets==3.5.1) (4.13.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==3.5.1) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==3.5.1) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==3.5.1) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==3.5.1) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==3.5.1) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==3.5.1) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.5.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.5.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.5.1) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.5.1) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.5.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.5.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.5.1) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.5.1) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets==3.5.1) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets==3.5.1) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets==3.5.1) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets==3.5.1) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets==3.5.1) (2024.2.0)\n",
      "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 3.5.0\n",
      "    Uninstalling datasets-3.5.0:\n",
      "      Successfully uninstalled datasets-3.5.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2025.3.0 which is incompatible.\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.5.1 fsspec-2025.3.0\n"
     ]
    }
   ],
   "source": [
    "# Install Packages\n",
    "!pip install datasets==3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T01:50:15.879464Z",
     "iopub.status.busy": "2025-06-06T01:50:15.879176Z",
     "iopub.status.idle": "2025-06-06T01:50:22.135536Z",
     "shell.execute_reply": "2025-06-06T01:50:22.134518Z",
     "shell.execute_reply.started": "2025-06-06T01:50:15.879441Z"
    },
    "id": "1KK4OTF7P8OH",
    "outputId": "8cee49f1-554e-4970-beb7-af63bd012588",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes==0.45.5\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes==0.45.5) (2.5.1+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes==0.45.5) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes==0.45.5) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes==0.45.5) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes==0.45.5) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes==0.45.5) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes==0.45.5) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes==0.45.5) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.5) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.5) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.5) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.5) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.5) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.5) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.5) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.5) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.5) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.5) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.5) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.5) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.5) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.5) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.5) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.5) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.5) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.5) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.5) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes==0.45.5) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes==0.45.5) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes==0.45.5) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes==0.45.5) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes==0.45.5) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes==0.45.5) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes==0.45.5) (2024.2.0)\n",
      "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.45.5\n"
     ]
    }
   ],
   "source": [
    "# Install Packages\n",
    "!pip install bitsandbytes==0.45.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rETWB371H85Q"
   },
   "source": [
    "# Load Dataset from huggingface\n",
    "Save GSM8K.json and ARC.json on Colab for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T01:50:22.137294Z",
     "iopub.status.busy": "2025-06-06T01:50:22.136890Z",
     "iopub.status.idle": "2025-06-06T01:50:27.847539Z",
     "shell.execute_reply": "2025-06-06T01:50:27.846713Z",
     "shell.execute_reply.started": "2025-06-06T01:50:22.137263Z"
    },
    "id": "lQ3HVDxXLMPf",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec74ef78dde49439f014862ea3bf652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615e51682dff4bef8a0acb7de2690115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GSM8K-00000-of-00001.parquet:   0%|          | 0.00/36.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a016f3b1e04928b417ced6fcc864a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ARC-00000-of-00001.parquet:   0%|          | 0.00/40.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3dd1b15c1c4e11931c903ee5e70a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating GSM8K split:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26695df438af4dc89cbf08eefcedeb8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating ARC split:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Dataset from huggingface\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "# This loads the default config with GSM8K and ARC as splits\n",
    "dataset = load_dataset(\"MonicaHuang/ML2025_HW9\")\n",
    "\n",
    "# Access the splits\n",
    "gsm8k = dataset[\"GSM8K\"]\n",
    "arc = dataset[\"ARC\"]\n",
    "gsm8k_path = \"./GSM8K.json\"\n",
    "arc_path = \"./ARC.json\"\n",
    "gsm8k_list = gsm8k.to_list()\n",
    "arc_list = arc.to_list()\n",
    "\n",
    "# Save datasets locally to Colab files\n",
    "with open(gsm8k_path, \"w\") as f:\n",
    "    json.dump(gsm8k_list, f, indent=2)\n",
    "\n",
    "with open(arc_path, \"w\") as f:\n",
    "    json.dump(arc_list, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAr00tFAIiEL"
   },
   "source": [
    "# **2025/05/21 Update**: This part is **\"Deprecated\"** to avoid concerns about plagiarism.\n",
    "\n",
    "Update peft package and install on Colab\n",
    "- There are two choices for modifying peft package. You can either:\n",
    "  1. Generate a private peft github repo from TA-version peft repo, clone and modify certain .py modules in peft to include your own merging methods in the package. Push your local modification to remote (private repo). \n",
    "    - Remember to generate your private repo access token and `pip install git+https://{username}:{token}@github.com/{username}/{private_peft_repo_name}.git`\n",
    "    - How to build a private customized peft package for accessing modified peft package on Colab/Kaggle?  \n",
    "    Tutorial Slide: https://docs.google.com/presentation/d/1tScnnXok48IBxnvQziysbv_johUdvdujsEQZrlk6xTU/edit?usp=sharing\n",
    "  2. Clone TA-version peft package, modify on Colab and install in editable mode (not recommend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T01:50:27.849904Z",
     "iopub.status.busy": "2025-06-06T01:50:27.848681Z",
     "iopub.status.idle": "2025-06-06T01:50:27.855840Z",
     "shell.execute_reply": "2025-06-06T01:50:27.854935Z",
     "shell.execute_reply.started": "2025-06-06T01:50:27.849869Z"
    },
    "id": "GGEQLFRmS9rN",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#### Choice 1\n",
    "#### TODO: Generate a private peft github repo from TA-version peft repo (Use this template), clone and modify certain .py modules in peft to include your own merging methods in the package. Push your local modification to remote (private repo).####\n",
    "########\n",
    "#### Submission: remember to submit all modified .py files in your peft packages for code reproduction ####\n",
    "########\n",
    "# Step 1: Set GitHub username and repo name\n",
    "# Step 2: Enter GitHub token securely (Optional)\n",
    "# Step 3: Construct repo URL and install via pip\n",
    "# Step 4: Install\n",
    "#### TODO: Replace variables with your own values ####\n",
    "# import os\n",
    "# from getpass import getpass\n",
    "# username = \"your_github_username\"\n",
    "# peft_repo = \"your_private_repo_name\"\n",
    "# token = getpass(\"Enter your GitHub token (it will be hidden): \")\n",
    "# os.environ[\"GITHUB_TOKEN\"] = token  # optional: store in env for reuse\n",
    "# repo_url = f\"git+https://{username}:{token}@github.com/{username}/{peft_repo}.git\"\n",
    "# !pip install {repo_url}\n",
    "\n",
    "\n",
    "# TA version peft package (public peft repo)\n",
    "# !pip install git+https://github.com/chenjoachim/peft-ml2025-hw9.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T01:50:27.857729Z",
     "iopub.status.busy": "2025-06-06T01:50:27.857401Z",
     "iopub.status.idle": "2025-06-06T01:50:27.877516Z",
     "shell.execute_reply": "2025-06-06T01:50:27.876742Z",
     "shell.execute_reply.started": "2025-06-06T01:50:27.857701Z"
    },
    "id": "Pm8gjl_iBWTw",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#### Choice 2 (not recommend)\n",
    "#### TODO: How to clone and modify peft package on Colab ####\n",
    "#### Submission: remember to submit all modified .py files in your peft packages for code reproduction ####\n",
    "# Step 1: Mount Drive for persistent use\n",
    "# Step 2: Make project folder\n",
    "# Step 3: Clone repo\n",
    "# Step 4: Move into repo\n",
    "# Step 5: Install in editable mode\n",
    "# Step 6: Add peft/src path to system path to successfully import peft package\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !mkdir -p /content/drive/MyDrive/ml2025_hw9\n",
    "# !git clone https://github.com/chenjoachim/peft-ml2025-hw9.git /content/drive/MyDrive/ml2025_hw9/peft\n",
    "\n",
    "# %cd /content/drive/MyDrive/ml2025_hw9/peft\n",
    "# !pip install -e .\n",
    "# %cd /content\n",
    "# import sys\n",
    "# sys.path.append(\"/content/drive/MyDrive/ml2025_hw9/peft/src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2025/05/21 Update**: \n",
    "# **Download, modify TA-version peft package and install on Colab/Kaggle**\n",
    "\n",
    "**Google Drive links** for downloading TA-version peft package (choose one)\n",
    "\n",
    "- Link1: https://drive.google.com/file/d/1HK8q4l7aMI6MjNdeJyzLCqbgM8lZCAZV/view?usp=sharing\n",
    "- Link2: https://drive.google.com/file/d/1eEtVtCjUj4HnAJLNh5nLp2ZZISYva-vH/view?usp=sharing\n",
    "- Link3: https://drive.google.com/file/d/1tzzsvwFzL4x6L76AG8ibKpB1dXEvHEW0/view?usp=sharing\n",
    "- Link4: https://drive.google.com/file/d/1SK9PxF23LdMnvKvi7q4R-R9iccQ9KLXX/view?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T01:50:27.878610Z",
     "iopub.status.busy": "2025-06-06T01:50:27.878328Z",
     "iopub.status.idle": "2025-06-06T01:50:27.902226Z",
     "shell.execute_reply": "2025-06-06T01:50:27.901678Z",
     "shell.execute_reply.started": "2025-06-06T01:50:27.878585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#### Mount Your Google Drive in Colab and create a directory(optional) for storing peft package\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !mkdir -p ./ml2025_hw9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T01:50:27.903234Z",
     "iopub.status.busy": "2025-06-06T01:50:27.902969Z",
     "iopub.status.idle": "2025-06-06T01:50:27.919763Z",
     "shell.execute_reply": "2025-06-06T01:50:27.919294Z",
     "shell.execute_reply.started": "2025-06-06T01:50:27.903212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#### Download and unzip TA-version peft package to a specific folder in your Google Drive\n",
    "#### or just modify your peft package locally and re-upload to a specific folder in Google Drive\n",
    "\n",
    "# # !pip install -U gdown\n",
    "# !gdown --id 1HK8q4l7aMI6MjNdeJyzLCqbgM8lZCAZV -O peft.zip\n",
    "\n",
    "# !gdown --id 1tzzsvwFzL4x6L76AG8ibKpB1dXEvHEW0 -O peft.zip\n",
    "# !gdown --id 1eEtVtCjUj4HnAJLNh5nLp2ZZISYva-vH -O peft.zip\n",
    "# !gdown --id 1SK9PxF23LdMnvKvi7q4R-R9iccQ9KLXX -O peft.zip\n",
    "\n",
    "# !unzip peft.zip -d ./ml2025_hw9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T01:50:27.922204Z",
     "iopub.status.busy": "2025-06-06T01:50:27.921982Z",
     "iopub.status.idle": "2025-06-06T01:50:27.938931Z",
     "shell.execute_reply": "2025-06-06T01:50:27.938400Z",
     "shell.execute_reply.started": "2025-06-06T01:50:27.922164Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#### Navigate to the root directory of the uploaded peft package and install it in editable mode after making your modifications.\n",
    "# Remember to add src (in peft package) directory to system path\n",
    "\n",
    "# %cd /kaggle/working/ml2025_hw9/peft-ml2025-hw9\n",
    "# !pip install -e .\n",
    "# %cd /kaggle/working\n",
    "# !ls\n",
    "# import sys\n",
    "# sys.path.append(\"./ml2025_hw9/peft-ml2025-hw9/src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you want to pip install your locally modified peft package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#### If you would like to pip install your own customized peft package: \n",
    "#### Step 1: upload the compressed peft package through kaggle Input with dataset named hw9-peft, \n",
    "#### Step 2: copy the peft folder from the dataset to /kaggle/working/, then run pip install -e .. \n",
    "#### Step 3: at last, add src directory to system path\n",
    "\n",
    "# Uncomment below\n",
    "!cp -r /kaggle/input/peft-ml2025-hw9/peft-ml2025-hw9 /kaggle/working/\n",
    "%cd /kaggle/working/peft-ml2025-hw9\n",
    "!pip install -e .\n",
    "\n",
    "%cd /kaggle/working\n",
    "!ls\n",
    "import sys\n",
    "sys.path.append(\"./peft-ml2025-hw9/src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-cRvhX6K4LI"
   },
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUF_KsB8fH-d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set deterministic cuBLAS behavior before importing torch\n",
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFoRuk2VviWD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import json\n",
    "import itertools\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import wandb\n",
    "import argparse\n",
    "import string\n",
    "\n",
    "from itertools import zip_longest\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel\n",
    "from transformers import (\n",
    "    GenerationConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer\n",
    ")\n",
    "import torch\n",
    "from peft import LoraConfig\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLCt94SuLD56"
   },
   "source": [
    "# Prompt Generation\n",
    "\n",
    "\n",
    "> You're not allowed to modify instructions, questions and options in HW9!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T01:51:21.002778Z",
     "iopub.status.busy": "2025-06-06T01:51:21.002026Z",
     "iopub.status.idle": "2025-06-06T01:51:21.009849Z",
     "shell.execute_reply": "2025-06-06T01:51:21.008956Z",
     "shell.execute_reply.started": "2025-06-06T01:51:21.002752Z"
    },
    "id": "Q8xb8rxVFkY1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prompt Generation, Data Preprocessor\n",
    "\n",
    "instruction_dict = {\n",
    "    \"GSM8K\": \"You are given a math question and four answer options (associated with \\\"A\\\", \\\"B\\\", \\\"C\\\", \\\"D\\\"). Your task is to carefully analyze the problem, apply logical reasoning, and select the correct answer. There is only one correct answer for each question.\",\n",
    "    \"ARC\": \"You are given a science question and four answer options (associated with \\\"A\\\", \\\"B\\\", \\\"C\\\", \\\"D\\\"). Your task is to find the correct answer based on scientific facts, knowledge, and reasoning. There is only one correct answer for each question.\",\n",
    "}\n",
    "\n",
    "def mcqa_prompt(task_name, instruction, question, options):\n",
    "    \"\"\"\n",
    "    Constructs a multiple-choice question answering (MCQA) prompt in instruction-tuning format.\n",
    "\n",
    "    Args:\n",
    "        task_name (str): The name of the task (e.g., \"GSM8K\", \"ARC\").\n",
    "        instruction (str): A task-specific instruction (not used directly in this function).\n",
    "        question (str): The question to be answered.\n",
    "        options (dict): A dictionary of answer options where keys are option IDs (e.g., \"A\", \"B\") and values are option texts.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted instruction-tuning prompt with system message, instruction, and user input.\n",
    "    \"\"\"\n",
    "\n",
    "    sys_msg = instruction_dict[task_name]\n",
    "    options_dict = options\n",
    "    IDs = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "    if \"D\" in options_dict:\n",
    "        op_ids = [IDs[i] for i in range(4)]\n",
    "    else:\n",
    "        op_ids = [IDs[i] for i in range(3)]\n",
    "    options = [options_dict[op_id] for op_id in op_ids]\n",
    "\n",
    "    user_prompt = f\"Question: {question}; Options: \" + \\\n",
    "        \" \".join([f\"({op_id}) {option}\" for op_id, option in zip(op_ids, options)])\n",
    "\n",
    "    return f\"\"\"[INST] <<SYS>>You are a helpful assistant and good at solving tasks based on task instructions and inputs.<</SYS>> Instruction: {sys_msg} Input: {user_prompt}[/INST]\"\"\"\n",
    "\n",
    "def generate_prompt(task_name, tokenizer, data_point):\n",
    "    \"\"\"\n",
    "    Generates a final prompt for the model by wrapping task-specific question and options.\n",
    "\n",
    "    Args:\n",
    "        task_name (str): The name of the dataset/task (e.g., \"GSM8K\", \"ARC\").\n",
    "        tokenizer: A tokenizer object (not used in this function but included for compatibility).\n",
    "        data_point (dict): A single example containing keys \"instruction\", \"question\", and \"options\".\n",
    "\n",
    "    Returns:\n",
    "        str: A fully constructed prompt ready to be tokenized and passed to the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    return mcqa_prompt(\n",
    "      task_name = task_name,\n",
    "      instruction = data_point[\"instruction\"],\n",
    "      question = data_point[\"question\"],\n",
    "      options = data_point[\"options\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xV-7vMzeLbry"
   },
   "source": [
    "# Seed Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T01:51:21.011140Z",
     "iopub.status.busy": "2025-06-06T01:51:21.010736Z",
     "iopub.status.idle": "2025-06-06T01:51:21.040255Z",
     "shell.execute_reply": "2025-06-06T01:51:21.039595Z",
     "shell.execute_reply.started": "2025-06-06T01:51:21.011115Z"
    },
    "id": "F9xQsLEgReyz",
    "outputId": "a884e820-b136-41e9-e4e8-625310632b99",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Sets the CUDA device to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T01:51:21.041121Z",
     "iopub.status.busy": "2025-06-06T01:51:21.040875Z",
     "iopub.status.idle": "2025-06-06T01:51:21.064659Z",
     "shell.execute_reply": "2025-06-06T01:51:21.063996Z",
     "shell.execute_reply.started": "2025-06-06T01:51:21.041104Z"
    },
    "id": "gRhWFJj3drLp",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Seed Settings\n",
    "torch.manual_seed(42)\n",
    "torch.use_deterministic_algorithms(True,warn_only=True)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cBYxbKZLfxk"
   },
   "source": [
    "# Merging Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T01:51:21.065691Z",
     "iopub.status.busy": "2025-06-06T01:51:21.065463Z",
     "iopub.status.idle": "2025-06-06T01:51:21.070274Z",
     "shell.execute_reply": "2025-06-06T01:51:21.069684Z",
     "shell.execute_reply.started": "2025-06-06T01:51:21.065675Z"
    },
    "id": "6Y4eqMxPkGza",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Merge Setting Constants and Paths\n",
    "# Current Merging Settings, including merging algorithm, density, or weights\n",
    "MERGE_TYPE = \"sce\"\n",
    "density = 0.3\n",
    "weight_math = 1.0\n",
    "weight_science = 0.25\n",
    "if weight_math and weight_science:\n",
    "  weights = [weight_math, weight_science]\n",
    "else:\n",
    "  weights = None\n",
    "\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "MERGE_TASK_NAMES = [\"GSM8K\", \"ARC\"] # Not recommend modifying as it's related to merging model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpVA8xKtL5LM"
   },
   "source": [
    "# Model Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T01:51:21.071073Z",
     "iopub.status.busy": "2025-06-06T01:51:21.070888Z",
     "iopub.status.idle": "2025-06-06T01:51:21.093701Z",
     "shell.execute_reply": "2025-06-06T01:51:21.093135Z",
     "shell.execute_reply.started": "2025-06-06T01:51:21.071060Z"
    },
    "id": "SntwsEEikNOx",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model Configs\n",
    "\"\"\"====== Model CONFIG======\"\"\"\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", 'k_proj', \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05\n",
    ")\n",
    "RANDOM_SEED = 42\n",
    "CUTOFF_LEN = 700"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eg342yI-MIB-"
   },
   "source": [
    "# Load Models and Tokenizers\n",
    "LoRA Weights of two tasks are loaded from huggingface.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T01:51:21.094609Z",
     "iopub.status.busy": "2025-06-06T01:51:21.094353Z",
     "iopub.status.idle": "2025-06-06T01:52:35.909706Z",
     "shell.execute_reply": "2025-06-06T01:52:35.908743Z",
     "shell.execute_reply.started": "2025-06-06T01:51:21.094584Z"
    },
    "id": "by_B7JnjkPzV",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Model and Tokenizer ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648aa2ef466b4b06bdc329737ee46ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b3903d90e14de3b9aa1795e9438243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be29b3fd6b244837aa14d2d50c162435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16027f514a4d4b159794155a6947fd8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ff7218adb6495d9afa62fc01dac111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4411117324d1459785e69b6456ea55d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d1b8cb32ab45208d00e9afed4cbb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/183 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapters: ['GSM8K', 'ARC']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10d6c6d0a904aacbf23e1028883c68e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/742 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80fb5e57f694089876feac1a65464f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/25.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ea3fc8ab97486a8dd401c50a44a9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/working/./peft-ml2025-hw9/src/peft/config.py:162: UserWarning: Unexpected keyword arguments ['corda_config', 'trainable_token_indices'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d918f4eee974616b632487017cfb811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/25.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Models and Tokenizers\n",
    "\"\"\" ====== Load Model and Tokenizer ====== \"\"\"\n",
    "print(\"Load Model and Tokenizer ...\")\n",
    "\n",
    "base_model_name = \"unsloth/llama-2-7b-chat-bnb-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name,\n",
    "        quantization_config=quant_config, use_fast=True)\n",
    "\n",
    "tokenizer.pad_token_id = 1\n",
    "tokenizer.eos_token_id = 2\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name,\n",
    "        quantization_config=quant_config, low_cpu_mem_usage=True, torch_dtype=torch.float16).to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "adapters = MERGE_TASK_NAMES\n",
    "print(f\"Adapters: {adapters}\")\n",
    "\n",
    "hf_model_dict = {\n",
    "    \"GSM8K\": \"MonicaHuang/llama-2-7b-chat-GSM8K-MCQA\",\n",
    "    \"ARC\": \"chenjoachim/llama-2-7b-chat-ARC-MCQA\",\n",
    "}\n",
    "\n",
    "# Load LoRA adapters, i.e. task vectors of two tasks\n",
    "model = PeftModel.from_pretrained(model, hf_model_dict[adapters[0]], adapter_name=adapters[0]).to(device)\n",
    "for adapter in adapters[1:]:\n",
    "    _ = model.load_adapter(hf_model_dict[adapter], adapter_name=adapter, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pStBkFyRfl3y"
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "# Reminders\n",
    "> Make sure to inference questions of two tasks under the same merging setting and Generation Config. Don't change settings or configs during the whole inference process.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7z-vz63fMyLd"
   },
   "source": [
    "# Merging with a specific algorithm in peft package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T01:52:35.911437Z",
     "iopub.status.busy": "2025-06-06T01:52:35.911117Z",
     "iopub.status.idle": "2025-06-06T01:52:37.469969Z",
     "shell.execute_reply": "2025-06-06T01:52:37.469175Z",
     "shell.execute_reply.started": "2025-06-06T01:52:35.911413Z"
    },
    "id": "TEN1KL8OkX1C",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Switch between Merging Algorithms\n",
    "weights = weights\n",
    "adapter_name =\"merge\"\n",
    "density = density\n",
    "\n",
    "if MERGE_TYPE == \"ties\":\n",
    "    model.add_weighted_adapter(adapters, weights, adapter_name, combination_type=\"ties\", density=density)\n",
    "    model.set_adapter(\"merge\")\n",
    "elif MERGE_TYPE == \"linear\": # task_arithmetic\n",
    "    model.add_weighted_adapter(adapters, weights, adapter_name, combination_type=\"linear\")\n",
    "    model.set_adapter(\"merge\")\n",
    "elif MERGE_TYPE == \"magnitude_prune\":\n",
    "    model.add_weighted_adapter(adapters, weights, adapter_name, combination_type=\"magnitude_prune\", density=density)\n",
    "    model.set_adapter(\"merge\")\n",
    "elif MERGE_TYPE == \"dare_ties\":\n",
    "    model.add_weighted_adapter(adapters, weights, adapter_name, combination_type=\"dare_ties\", density=density)\n",
    "    model.set_adapter(\"merge\")\n",
    "elif MERGE_TYPE == \"dare_linear\":\n",
    "    model.add_weighted_adapter(adapters, weights, adapter_name, combination_type=\"dare_linear\", density=density)\n",
    "    model.set_adapter(\"merge\")\n",
    "\n",
    "########\n",
    "#### TODO: append newly integrated methods here (to access peft package) ####\n",
    "########\n",
    "elif MERGE_TYPE == \"sce\":\n",
    "    model.add_weighted_adapter(adapters, weights, adapter_name, combination_type=\"sce\", density=density, majority_sign_method=\"total\")\n",
    "    model.set_adapter(\"merge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVbx9fIQNKEQ"
   },
   "source": [
    "# Generation Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T01:52:37.470984Z",
     "iopub.status.busy": "2025-06-06T01:52:37.470711Z",
     "iopub.status.idle": "2025-06-06T01:52:37.849782Z",
     "shell.execute_reply": "2025-06-06T01:52:37.848858Z",
     "shell.execute_reply.started": "2025-06-06T01:52:37.470953Z"
    },
    "id": "xA3MEacQAmC6",
    "outputId": "e84dc750-d6bb-41cc-a322-85176b115bc5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Generation Config ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\" ====== Generation Config ====== \"\"\"\n",
    "print(\"Setting Generation Config ...\")\n",
    "#######\n",
    "#### TODO: Hyperparameter Tuing ####\n",
    "#######\n",
    "\n",
    "hyperparameters = {\n",
    "    \"do_sample\": False, #True,\n",
    "    \"temperature\": None, #0.1,\n",
    "    \"num_beams\": 1,\n",
    "    \"top_p\": None, #0.3,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"max_new_len\": 600\n",
    "}\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample=hyperparameters[\"do_sample\"],\n",
    "    temperature=hyperparameters[\"temperature\"],\n",
    "    top_p=hyperparameters[\"top_p\"],\n",
    "    num_beams=hyperparameters[\"num_beams\"],\n",
    "    pad_token_id=1,\n",
    "    max_new_tokens=hyperparameters[\"max_new_len\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onL5DsrghFvs"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3wGB_1JOXY2"
   },
   "source": [
    "# Math - GSM8K Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T01:52:37.851315Z",
     "iopub.status.busy": "2025-06-06T01:52:37.850995Z"
    },
    "id": "nCO7Od1TkgSp",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Test Dataset GSM8K ...\n",
      "Inference ... GSM8K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8/200 [03:34<1:32:26, 28.89s/it]"
     ]
    }
   ],
   "source": [
    "# GSM8K Inference Pipeline\n",
    "TEST_TASK = \"GSM8K\"\n",
    "TEST_SET_PATH = f\"{TEST_TASK}.json\"\n",
    "\n",
    "print(f\"Load Test Dataset {TEST_TASK} ...\")\n",
    "with open(TEST_SET_PATH, \"r\", encoding = \"utf-8\") as f:\n",
    "    test_datas = json.load(f)\n",
    "\n",
    "print(f\"Inference ... {TEST_TASK}\")\n",
    "\n",
    "results = []\n",
    "max_iteration = len(test_datas)\n",
    "for (i, test_data) in tqdm(enumerate(test_datas[:max_iteration]), total = max_iteration):\n",
    "\n",
    "    \"\"\" ====== Inference ======= \"\"\"\n",
    "    prompt = generate_prompt(TEST_TASK, tokenizer, test_data)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      generation_output = model.generate(\n",
    "          input_ids=input_ids,\n",
    "          generation_config=generation_config,\n",
    "          return_dict_in_generate=True,\n",
    "          output_scores=True,\n",
    "          max_new_tokens=hyperparameters[\"max_new_len\"],\n",
    "      )\n",
    "\n",
    "    for s in generation_output.sequences:\n",
    "        predict = tokenizer.decode(s)\n",
    "\n",
    "    response = predict.split(\"[/INST]\")[-1].split('</s>')[0].strip()\n",
    "    #### Hints: output some generated responses ####\n",
    "    # print(f\"response: {response}\")\n",
    "    results.append({\n",
    "        \"id\": test_data[\"id\"],\n",
    "        \"response\": response,\n",
    "    })\n",
    "    \"\"\" ====== Inference End ====== \"\"\"\n",
    "\n",
    "# Save Response Records\n",
    "\"\"\"====== Records ======\"\"\"\n",
    "if weights:\n",
    "  wnames = '_'.join([str(float(w)) for w in weights])\n",
    "else:\n",
    "  wnames = None\n",
    "\n",
    "result_dir = f\"{OUTPUT_DIR}/{MERGE_TYPE}/{TEST_TASK}\"\n",
    "\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "with open(f\"{result_dir}/w_{wnames}_d_{density}_result.json\",\"w\", encoding = \"utf-8\") as f:\n",
    "    json.dump(results, f, indent = 2, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4Xq8Wf4OgPa"
   },
   "source": [
    "# Science - ARC Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXvpjn0-uFwg",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ARC Inference Pipeline\n",
    "TEST_TASK = \"ARC\"\n",
    "TEST_SET_PATH = f\"{TEST_TASK}.json\"\n",
    "\n",
    "\"\"\" ====== Load Test Dataset ====== \"\"\"\n",
    "print(f\"Load Test Dataset {TEST_TASK} ...\")\n",
    "with open(TEST_SET_PATH, \"r\", encoding = \"utf-8\") as f:\n",
    "    test_datas = json.load(f)\n",
    "\n",
    "print(f\"Inference ...{TEST_TASK}\")\n",
    "\n",
    "results = []\n",
    "max_iteration = len(test_datas)\n",
    "for (i, test_data) in tqdm(enumerate(test_datas[:max_iteration]), total = max_iteration):\n",
    "\n",
    "    \"\"\" ====== Inference ======= \"\"\"\n",
    "    prompt = generate_prompt(TEST_TASK, tokenizer, test_data)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      generation_output = model.generate(\n",
    "          input_ids=input_ids,\n",
    "          generation_config=generation_config,\n",
    "          return_dict_in_generate=True,\n",
    "          output_scores=True,\n",
    "          max_new_tokens=hyperparameters[\"max_new_len\"],\n",
    "      )\n",
    "\n",
    "    for s in generation_output.sequences:\n",
    "        predict = tokenizer.decode(s)\n",
    "\n",
    "    response = predict.split(\"[/INST]\")[-1].split('</s>')[0].strip()\n",
    "    #### Hints: output some generated responses ####\n",
    "    # print(f\"response: {response}\")\n",
    "    results.append({\n",
    "        \"id\": test_data[\"id\"],\n",
    "        \"response\": response\n",
    "    })\n",
    "    \"\"\" ====== Inference End ====== \"\"\"\n",
    "\n",
    "\n",
    "\"\"\"====== Records ======\"\"\"\n",
    "if weights:\n",
    "  wnames = '_'.join([str(float(w)) for w in weights])\n",
    "else:\n",
    "  wnames = None\n",
    "result_dir = f\"{OUTPUT_DIR}/{MERGE_TYPE}/{TEST_TASK}\"\n",
    "\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "with open(f\"{result_dir}/w_{wnames}_d_{density}_result.json\",\"w\", encoding = \"utf-8\") as f:\n",
    "    json.dump(results, f, indent = 2, ensure_ascii = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVAEz4uDOlwS"
   },
   "source": [
    "# Generate Judgeboi Submission File\n",
    "\n",
    "The prediction file (pred.json) must follow this structure:\n",
    "- Root must be a dictionary ({}).\n",
    "- Each key must be a string representing an ID (e.g., \"arc_1\", \"gsm8k_32\").\n",
    "- Each value must be a string containing the model-generated response without input prompt.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5RqbOAVlDXm",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generate Judgeboi Submission files\n",
    "# 400 responses in pred.json\n",
    "import json\n",
    "#### TODO: The response generation of two tasks must be under same merging setting!!!!! ####\n",
    "arc_path = f\"outputs/{MERGE_TYPE}/ARC/w_{wnames}_d_{density}_result.json\" # Update the path with your arc file path(absolute)\n",
    "gsm8k_path = f\"outputs/{MERGE_TYPE}/GSM8K/w_{wnames}_d_{density}_result.json\" # Update the path with your gsm8k file path(absolute)\n",
    "output_path = \"pred.json\" # name of outpuft file is fixed\n",
    "\n",
    "def load_list_as_dict(filepath):\n",
    "    \"\"\"\n",
    "    Loads a list of {\"id\": ..., \"response\": ...} and converts it to {id: response}\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    output = {}\n",
    "    for item in data:\n",
    "        if 'id' in item and 'response' in item:\n",
    "            output[item['id']] = item['response']\n",
    "        else:\n",
    "            print(f\"[Error] Skipping item without valid 'id' and 'response': {item}\")\n",
    "    return output\n",
    "\n",
    "gsm8k_dict = load_list_as_dict(gsm8k_path)\n",
    "arc_dict = load_list_as_dict(arc_path)\n",
    "\n",
    "combined = {**gsm8k_dict, **arc_dict}\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(combined, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vo3LVLm9ebu4"
   },
   "source": [
    "## End of HW9 Notebook\n",
    "Thank you for reading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfzmlZr4mrJ3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Appendix: Batch Decoding/Generation\n",
    "'''\n",
    "def evaluate_batch(task_name, model, tokenizer, data_points, generation_config, max_len, verbose=True):\n",
    "\n",
    "    prompts = [generate_prompt(task_name, tokenizer, dp) for dp in data_points]\n",
    "\n",
    "    # Batch tokenization\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=CUTOFF_LEN)\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_len,\n",
    "        )\n",
    "\n",
    "    outputs = tokenizer.batch_decode(generation_output.sequences, skip_special_tokens=True)\n",
    "\n",
    "    if verbose:\n",
    "        for idx, output in enumerate(outputs):\n",
    "            print(\"================= Response of Model ==================\")\n",
    "            print({\n",
    "                \"input_prompt\": prompts[idx],\n",
    "                \"output_id\": generation_output.sequences[idx],\n",
    "                \"output\": output,\n",
    "            })\n",
    "\n",
    "    return outputs\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "config = {\n",
    "    \"batch_size\": 4\n",
    "}\n",
    "results = []\n",
    "max_iteration = len(test_datas)\n",
    "BATCH_SIZE = config[\"batch_size\"]\n",
    "\n",
    "for batch_start in tqdm(range(0, max_iteration, BATCH_SIZE), total=(max_iteration // BATCH_SIZE)+1):\n",
    "    batch_data = test_datas[batch_start:batch_start+BATCH_SIZE]\n",
    "\n",
    "    predicts = evaluate_batch(\n",
    "        TEST_TASK,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        batch_data,\n",
    "        generation_config,\n",
    "        hyperparameters[\"max_new_len\"],\n",
    "        verbose = False\n",
    "    )\n",
    "\n",
    "    for predict, test_data in zip(predicts, batch_data):\n",
    "        response = predict.split(\"[/INST]\")[-1].split('</s>')[0].strip()\n",
    "\n",
    "\n",
    "        results.append({\n",
    "            \"id\": test_data[\"id\"],\n",
    "            \"response\": response\n",
    "        })\n",
    "        \"\"\" ====== Inference End ====== \"\"\"\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Vo3LVLm9ebu4"
   ],
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7601334,
     "sourceId": 12075666,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7601977,
     "sourceId": 12076591,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
